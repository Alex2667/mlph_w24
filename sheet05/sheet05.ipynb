{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Names: Philipp KÃ¶hler, Alexander Bespalov**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 The logistic sigmoid\n",
    "\n",
    "### (a)\n",
    "Sigmoid function:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "Derivative of the sigmoid (quotient rule used):\n",
    "$$\n",
    "\\frac{d}{dx} \\sigma(x) = \\frac{e^{-x}}{(1 + e^{-x})^2}\n",
    "$$\n",
    "Use:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "1 - \\sigma(x) = \\frac{e^{-x}}{1 + e^{-x}}\n",
    "$$\n",
    "Thus:\n",
    "$$\n",
    "\\frac{d}{dx} \\sigma(x) = \\sigma(x) (1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "### (b)\n",
    "\n",
    "The hyperbolic tangent function is:\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "The logistic sigmoid function is:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Rewrite $\\tanh(x)$ by factoring $e^{-x}$ from numerator and denominator:\n",
    "$$\n",
    "\\tanh(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}}\n",
    "$$\n",
    "\n",
    "From $\\sigma(x)$, we can write:\n",
    "$$\n",
    "1 - \\sigma(x) = \\frac{e^{-x}}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "\\sigma(x) - (1 - \\sigma(x))= 2\\sigma(x) - 1 = \\frac{2}{1 + e^{-x}} - 1 = \\frac{1 - e^{-x}}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Observe that $\\tanh(x)$ matches $2\\sigma(2x) - 1$.\n",
    "\n",
    "### (c)\n",
    "The two classes are separated by a line $w^Tx+b$ that goes through the points $x_1 = (1,1.5)$ and $x_2 = (2,2.5)$. The points are chosen because they are in the middle between the datapoints of the two classes. The resulting weights are given by $w = (1,-1)$ and the bias $b=0.5$. Testing those weights on the data gives values below 0 for class 2 and above 0 for class 1 in the argument of the sigmoid. Therefore the activation separates the classes with the given weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Logistic regression: an LLM lie detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data from https://heibox.uni-heidelberg.de/f/38bd3f2a9b7944248cc2/   \n",
    "Unzip it and place the lie_detection folder in the folder named `data` to get the following structure:\n",
    "\"data/lie_detection/datasets\" and \"data/lie_detection/acts\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you can load a dataset of LLM activations. Use a new Datamanager if you want to have a new dataset. Use the same data manager if you want to combine datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lie_detection_utils import DataManager\n",
    "\n",
    "path_to_datasets = \"data/lie_detection/datasets\"\n",
    "path_to_acts = \"data/lie_detection/acts\"\n",
    "\n",
    "# check if the datasets and activations are available\n",
    "assert os.path.exists(path_to_datasets), \"The path to the datasets does not exist.\"\n",
    "assert os.path.exists(path_to_acts), \"The path to the activations does not exist.\"\n",
    "\n",
    "# these are the different datasets containing true and false factual statements about different topics\n",
    "dataset_names = [\"cities\", \"neg_cities\", \"sp_en_trans\", \"neg_sp_en_trans\"]\n",
    "dataset_name = dataset_names[0] # choose some dataset from the above datasets, index \"0\" loads the \"cities\" dataset for example\n",
    "\n",
    "# the dataloader automatically loads the training data for us\n",
    "dm = DataManager()\n",
    "dm.add_dataset(dataset_name, \"Llama3\", \"8B\", \"chat\", layer=12, split=0.8, center=False,\n",
    "                device='cpu', path_to_datasets=path_to_datasets, path_to_acts=path_to_acts)\n",
    "acts_train, labels_train = dm.get('train') # train set\n",
    "acts_test, labels_test = dm.get('val')\n",
    "print(acts_train.shape, labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the statements that were fed to the LLM to produce the activations:\n",
    "df = pd.read_csv(f\"{path_to_datasets}/{dataset_name}.csv\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Log-sum-exp and soft(arg)max\n",
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Linear regions of MLPs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
